{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9aqRHfFkpJr",
        "outputId": "69b9b83a-d121-46e4-e118-36e837807f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV3ck1wXJ3xL"
      },
      "source": [
        "# Task 1: Using RLTK to perform Entity Resolution (ER)\n",
        "\n",
        "<sub>Content of this notebook was prepared by Basel Shbita, and modified by Avijit Thawani (thawani@usc.edu) as part of the class <u>DSCI 558: Building Knowledge Graphs</u> at University of Southern California (USC).</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYLgCLPJ3xN"
      },
      "source": [
        "The Record Linkage ToolKit ([RLTK](https://github.com/usc-isi-i2/rltk)) is a general-purpose open-source record linkage platform that allows users to build powerful Python programs that link records referring to the same underlying entity.\n",
        "\n",
        "This notebook introduces some applied examples using RLTK. You can also find additional examples and use-cases in [RLTK's documentation](https://rltk.readthedocs.io/en/master/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2Q5FfqkJ3xN"
      },
      "source": [
        "## Dataset analysis & RLTK components construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTlKyPqCJ3xO"
      },
      "outputs": [],
      "source": [
        "!pip install rltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8fyiwzJJ3xP"
      },
      "source": [
        "### Task 1-1. Construct RLTK Datasets\n",
        "\n",
        "First, you need define how a single entry would like for each type of record (for each dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "We7cQGTRJ3xP"
      },
      "outputs": [],
      "source": [
        "import rltk\n",
        "import csv\n",
        "\n",
        "# You can use this tokenizer in case you need to manipulate some data\n",
        "tokenizer = rltk.tokenizer.crf_tokenizer.crf_tokenizer.CrfTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T3Q_rPqmJ3xP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def get_singer(singer_str, isCosinger):\n",
        "  singers = re.split(' ft. | feat. | & | featuring | - |, | and | & ', singer_str)\n",
        "  if not singers:\n",
        "    return singers\n",
        "  return singers[1:] if isCosinger else singers[0]\n",
        "\n",
        "class SendhandRecord(rltk.Record):\n",
        "    def __init__(self, raw_object):\n",
        "        super().__init__(raw_object)\n",
        "    \n",
        "\n",
        "    @rltk.cached_property\n",
        "    def id(self):\n",
        "        return self.raw_object['index']\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def song(self):\n",
        "        return self.raw_object['title']\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def singer(self): \n",
        "      return get_singer(self.raw_object['originally_by'], isCosinger=False)\n",
        "    \n",
        "    @rltk.cached_property\n",
        "    def singer_tokens(self):\n",
        "        singer = get_singer(self.raw_object['originally_by'], isCosinger=False)\n",
        "        return set(tokenizer.tokenize(singer))\n",
        "    \n",
        "    @rltk.cached_property\n",
        "    def singer_first_name(self):\n",
        "      return get_singer(self.raw_object['originally_by'], isCosinger=False).split(' ')[0]\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def co_singer(self): \n",
        "      return get_singer(self.raw_object['originally_by'], isCosinger=True)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "class WikiRecord(rltk.Record):\n",
        "    def __init__(self, raw_object):\n",
        "        super().__init__(raw_object)\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def id(self):\n",
        "        return self.raw_object['song'].split('/')[-1]\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def song(self):\n",
        "        return self.raw_object['songLabel']\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def singer(self): \n",
        "      return get_singer(self.raw_object['performerLabel'], isCosinger=False)\n",
        "    \n",
        "    @rltk.cached_property\n",
        "    def singer_tokens(self):\n",
        "        singer = get_singer(self.raw_object['performerLabel'], isCosinger=False)\n",
        "        return set(tokenizer.tokenize(singer))\n",
        "    \n",
        "    @rltk.cached_property\n",
        "    def singer_first_name(self):\n",
        "      return get_singer(self.raw_object['performerLabel'], isCosinger=False).split(' ')[0]\n",
        "\n",
        "    @rltk.cached_property\n",
        "    def co_singer(self): \n",
        "      return get_singer(self.raw_object['performerLabel'], isCosinger=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cZ2VHWnwJ3xQ"
      },
      "outputs": [],
      "source": [
        "dir_ = ''\n",
        "sendhand_file = dir_ + 'original_song_stats.csv'\n",
        "wiki_file = dir_ + 'nominated_or_received_awards.csv'\n",
        "\n",
        "ds1 = rltk.Dataset(rltk.CSVReader(sendhand_file),record_class=SendhandRecord)\n",
        "ds2 = rltk.Dataset(rltk.CSVReader(wiki_file),record_class=WikiRecord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XHgnuhJJ3xQ"
      },
      "source": [
        "You can load your csv files into RLTK using this method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIylouixJ3xQ"
      },
      "source": [
        "And we can inspect a few entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5CUrpjGTJ3xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb0ddcd-f25c-407b-9067-fed3cd9a132d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  id               song        singer    singer_tokens singer_first_name  \\\n",
            "0  1          Let It Go  Idina Menzel  {Idina, Menzel}             Idina   \n",
            "1  2          All of Me   John Legend   {Legend, John}              John   \n",
            "2  3              Hello         Adele          {Adele}             Adele   \n",
            "3  4  Thinking Out Loud    Ed Sheeran    {Ed, Sheeran}                Ed   \n",
            "4  5            Perfect    Ed Sheeran    {Ed, Sheeran}                Ed   \n",
            "\n",
            "  co_singer  \n",
            "0        []  \n",
            "1        []  \n",
            "2        []  \n",
            "3        []  \n",
            "4        []  \n",
            "           id             song           singer       singer_tokens  \\\n",
            "0   Q16323774      Intelligent      Raske Penge      {Raske, Penge}   \n",
            "1   Q17628069        Q17628069              C2C               {C2C}   \n",
            "2  Q109659406       Q109659406  Svetlana Loboda  {Svetlana, Loboda}   \n",
            "3     Q858020  Frontside Ollie   Robin Packalen   {Packalen, Robin}   \n",
            "4    Q3878196  Non Ã¨ l'inferno     Emma Marrone     {Emma, Marrone}   \n",
            "\n",
            "  singer_first_name co_singer  \n",
            "0             Raske        []  \n",
            "1               C2C        []  \n",
            "2          Svetlana        []  \n",
            "3             Robin        []  \n",
            "4              Emma        []  \n"
          ]
        }
      ],
      "source": [
        "# print some entries\n",
        "print(ds1.generate_dataframe().head(5))\n",
        "print(ds2.generate_dataframe().head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB0HHqDpJ3xR"
      },
      "source": [
        "### Task 1-2. Blocking\n",
        "\n",
        "First, we'll load dev set to evaluate both blocking (Task 1-2) and entity linking (Task 1-3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "IxPPksRA27D2"
      },
      "outputs": [],
      "source": [
        "# Block by Initialisms of singer name\n",
        "bg = rltk.HashBlockGenerator()\n",
        "block = bg.generate(\n",
        "      bg.block(ds1, function_=lambda r: ''.join(r.singer.split(' '))), \n",
        "      bg.block(ds2, function_=lambda r: ''.join(r.singer.split(' '))))\n",
        "\n",
        "# BLock by song name\n",
        "bg2 = rltk.HashBlockGenerator()\n",
        "block2 = bg2.generate(\n",
        "      # bg2.block(ds1, function_=lambda r: r.publish_date[-4:], base_on=block), \n",
        "      # bg2.block(ds2, function_=lambda r: r.publish_date[-4:], base_on=block)\n",
        "      bg2.block(ds1, function_=lambda r: r.song[0], base_on=block), \n",
        "      bg2.block(ds2, function_=lambda r: r.song[0], base_on=block)\n",
        "      )\n",
        "\n",
        "# bg3 = rltk.HashBlockGenerator()\n",
        "# block3 = bg3.generate(\n",
        "#       bg3.block(ds1, function_=lambda r: r.isbn), \n",
        "#       bg3.block(ds2, function_=lambda r: r.isbn))\n",
        "\n",
        "pairs = rltk.get_record_pairs(ds1, ds2, block=block2)\n",
        "# for r1, r2 in pairs:\n",
        "#   print(r1.id, r1.author_first_name, r1.publish_date, '\\t', r2.id, r2.author_first_name, r2.publish_date)\n",
        "\n",
        "matched_wiki_ids = [] # for later ground truth evaluation\n",
        "pairs_pred = []\n",
        "with open(dir_+'blocked.csv', 'w+') as f:\n",
        "  # write header\n",
        "  f.write('secondhandsong_ID, wiki_nominated_ID\\n')\n",
        "  for r1, r2 in pairs:\n",
        "    # print(r1.id, r1.author_first_name, r1.publish_date, '\\t', r2.id, r2.author_first_name, r2.publish_date)\n",
        "    pairs_pred += [(r1.id, r2.id)]\n",
        "    f.write(f'{r1.id},{r2.id}\\n')\n",
        "    # print(f'{r1.id}, {r2.id}\\n')\n",
        "\n",
        "    matched_wiki_ids += [r2.id]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-construct ground truth file"
      ],
      "metadata": {
        "id": "HMlpKwsjumOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "import random\n",
        "random.seed(123)\n",
        "\n",
        "secondhand_df = ds1.generate_dataframe()\n",
        "secondhand_ids = list(secondhand_df['id'].values)\n",
        "\n",
        "wiki_df = ds2.generate_dataframe()\n",
        "wiki_ids = list(wiki_df['id'].values)\n",
        "# wiki_ids = non_matched_wiki_ids\n",
        "\n",
        "secondhand_wiki_ids = []\n",
        "for secondhand_id in secondhand_ids:\n",
        "  for wiki_id in wiki_ids:\n",
        "    secondhand_wiki_ids += [(secondhand_id, wiki_id)]\n",
        "print(f'combinations Length = {len(secondhand_wiki_ids)}')\n",
        "\n",
        "selected_secondhand_ids = random.sample(secondhand_wiki_ids, 100)\n",
        "selected_secondhand_ids\n",
        "\n",
        "with open(dir_+'matching_ground_truth.csv', 'w+') as f:\n",
        "  f.write('secondhandsong_ID,wiki_nominated_ID,label')\n",
        "\n",
        "  for (secondhand_id, wiki_id) in selected_secondhand_ids:\n",
        "    f.write(f'\\n{secondhand_id},{wiki_id},')"
      ],
      "metadata": {
        "id": "2XithLWVuqGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49c8707-6feb-42b8-9ccd-dace0e4e0104"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combinations Length = 1452900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "merged_dict = {'wiki_id':[], 'wiki_song':[], 'wiki_singer':[],\n",
        "               'secondhand_id':[], 'secondhand_song':[], 'secondhand_singer':[]}\n",
        "\n",
        "for (secondhand_id, wiki_id) in selected_secondhand_ids:\n",
        "  wiki_song = wiki_df.query(f'id==\"{wiki_id}\"').song.values[0]\n",
        "  wiki_singer = wiki_df.query(f'id==\"{wiki_id}\"').singer.values[0]\n",
        "  \n",
        "  secondhand_song = secondhand_df.query(f'id==\"{secondhand_id}\"').song.values[0]\n",
        "  secondhand_singer = secondhand_df.query(f'id==\"{secondhand_id}\"').singer.values[0]\n",
        "\n",
        "  for (col, val) in [('wiki_id', wiki_id), ('wiki_song', wiki_song), ('wiki_singer', wiki_singer),\n",
        "                     ('secondhand_id', secondhand_id), ('secondhand_song', secondhand_song), ('secondhand_singer', secondhand_singer)]:\n",
        "    merged_dict[col] += [val]\n",
        "\n",
        "merged_df = pd.DataFrame(merged_dict)\n",
        "merged_df.to_csv(dir_+'Entity Linking/'+'check.csv')\n",
        "merged_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Z-vlVlxWxzf2",
        "outputId": "170872fe-9a87-482b-dd33-d059d1aae095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       wiki_id       wiki_song         wiki_singer secondhand_id  \\\n",
              "0    Q19856909  Maria Salvador                J-Ax           758   \n",
              "1   Q114704064      Q114704064  Filipe Escandurras          3872   \n",
              "2   Q108805763      Easy on Me               Adele          1261   \n",
              "3   Q112774463           Taksi              Kalush          5890   \n",
              "4    Q15131296       Erilaiset      Robin Packalen          3856   \n",
              "..         ...             ...                 ...           ...   \n",
              "95  Q106970624        MiÃ©nteme       MarÃ­a Becerra          5868   \n",
              "96   Q19856909  Maria Salvador                J-Ax          7695   \n",
              "97   Q13461348      Formidable             Stromae          7247   \n",
              "98   Q97153993    Head & Heart          Joel Corry           864   \n",
              "99  Q107494555    Quatre Mains                Deus          9240   \n",
              "\n",
              "                secondhand_song  \\\n",
              "0                     That Girl   \n",
              "1               The Only Reason   \n",
              "2                     Fire Away   \n",
              "3     Why You Gotta Be That Way   \n",
              "4                          Wave   \n",
              "..                          ...   \n",
              "95  Making It Up as We Go Along   \n",
              "96                     Hele deg   \n",
              "97        Dela min drÃ¶m med mej   \n",
              "98                    Taki taki   \n",
              "99            CÃ®ntec franÈuzesc   \n",
              "\n",
              "                                    secondhand_singer  \n",
              "0                                                 J T  \n",
              "1                                 5 Seconds of Summer  \n",
              "2                                     Chris Stapleton  \n",
              "3                                         Ina Forsman  \n",
              "4                                                Beck  \n",
              "..                                                ...  \n",
              "95              Kristen Anderson-Lopez & Robert Lopez  \n",
              "96  Ulrikke Brandstorp, Unn Vibeke Hol, Nahom Fesh...  \n",
              "97                                      Lasse Stefanz  \n",
              "98          DJ Snake - Selena Gomez - Ozuna - Cardi B  \n",
              "99                                  Alexandru AndrieÅ  \n",
              "\n",
              "[100 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fb62e93-7b76-4d0f-b9b4-75d1e5799f29\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wiki_id</th>\n",
              "      <th>wiki_song</th>\n",
              "      <th>wiki_singer</th>\n",
              "      <th>secondhand_id</th>\n",
              "      <th>secondhand_song</th>\n",
              "      <th>secondhand_singer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q19856909</td>\n",
              "      <td>Maria Salvador</td>\n",
              "      <td>J-Ax</td>\n",
              "      <td>758</td>\n",
              "      <td>That Girl</td>\n",
              "      <td>J T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q114704064</td>\n",
              "      <td>Q114704064</td>\n",
              "      <td>Filipe Escandurras</td>\n",
              "      <td>3872</td>\n",
              "      <td>The Only Reason</td>\n",
              "      <td>5 Seconds of Summer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q108805763</td>\n",
              "      <td>Easy on Me</td>\n",
              "      <td>Adele</td>\n",
              "      <td>1261</td>\n",
              "      <td>Fire Away</td>\n",
              "      <td>Chris Stapleton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q112774463</td>\n",
              "      <td>Taksi</td>\n",
              "      <td>Kalush</td>\n",
              "      <td>5890</td>\n",
              "      <td>Why You Gotta Be That Way</td>\n",
              "      <td>Ina Forsman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q15131296</td>\n",
              "      <td>Erilaiset</td>\n",
              "      <td>Robin Packalen</td>\n",
              "      <td>3856</td>\n",
              "      <td>Wave</td>\n",
              "      <td>Beck</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Q106970624</td>\n",
              "      <td>MiÃ©nteme</td>\n",
              "      <td>MarÃ­a Becerra</td>\n",
              "      <td>5868</td>\n",
              "      <td>Making It Up as We Go Along</td>\n",
              "      <td>Kristen Anderson-Lopez &amp; Robert Lopez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Q19856909</td>\n",
              "      <td>Maria Salvador</td>\n",
              "      <td>J-Ax</td>\n",
              "      <td>7695</td>\n",
              "      <td>Hele deg</td>\n",
              "      <td>Ulrikke Brandstorp, Unn Vibeke Hol, Nahom Fesh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Q13461348</td>\n",
              "      <td>Formidable</td>\n",
              "      <td>Stromae</td>\n",
              "      <td>7247</td>\n",
              "      <td>Dela min drÃ¶m med mej</td>\n",
              "      <td>Lasse Stefanz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Q97153993</td>\n",
              "      <td>Head &amp; Heart</td>\n",
              "      <td>Joel Corry</td>\n",
              "      <td>864</td>\n",
              "      <td>Taki taki</td>\n",
              "      <td>DJ Snake - Selena Gomez - Ozuna - Cardi B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Q107494555</td>\n",
              "      <td>Quatre Mains</td>\n",
              "      <td>Deus</td>\n",
              "      <td>9240</td>\n",
              "      <td>CÃ®ntec franÈuzesc</td>\n",
              "      <td>Alexandru AndrieÅ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fb62e93-7b76-4d0f-b9b4-75d1e5799f29')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fb62e93-7b76-4d0f-b9b4-75d1e5799f29 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fb62e93-7b76-4d0f-b9b4-75d1e5799f29');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blocking Evaluatiom"
      ],
      "metadata": {
        "id": "uLM_oN66IHZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfGah0DhJ3xR",
        "outputId": "a424c897-ef2a-437d-d0f7-790e1f9b042b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names are: wiki_id, secondhand_id, label\n",
            "Processed 100 lines.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<rltk.evaluation.trial.Trial at 0x7f0149b84dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "dev_set_file = dir_ + 'matching_ground_truth.csv'\n",
        "dev = []\n",
        "columns = None\n",
        "with open(dev_set_file, encoding='utf-8', errors=\"replace\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "        # Empty row\n",
        "        if not row: continue\n",
        "        if not columns:\n",
        "            columns = row\n",
        "        else:\n",
        "            dev.append(row)\n",
        "    print(f'Column names are: {\", \".join(columns)}')\n",
        "    print(f'Processed {len(dev)} lines.')\n",
        "\n",
        "\n",
        "pairs_groundtrue = []\n",
        "gt = rltk.GroundTruth()\n",
        "for row in dev:    \n",
        "    wiki_ID,\tsecondhand_ID, label = row\n",
        "    r1 = ds1.get_record(secondhand_ID)\n",
        "    r2 = ds2.get_record(wiki_ID)\n",
        "    if label == '1':\n",
        "        secondhand_id = r1.raw_object['index']\n",
        "        wiki_id = r2.raw_object['song'].split('/')[-1]\n",
        "        gt.add_positive(secondhand_id, wiki_id)\n",
        "        pairs_groundtrue += [(secondhand_id, wiki_id)]\n",
        "    else:\n",
        "        gt.add_negative(secondhand_id, wiki_id)\n",
        "\n",
        "rltk.Trial(gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ07ud86J3xR"
      },
      "source": [
        "Then, you can build your own blocking techniques and evaluate it.\n",
        "\n",
        "Hint:\n",
        "\n",
        "- What is the total number of pairs without blocking? \n",
        "- what is the number of paris with blocking?\n",
        "- After blocking, how many \"correct\" (matched) pairs presented in dev set?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyl5cfThHTkm",
        "outputId": "a0d3fb6a-b5f7-43e6-dd28-ce039b07545e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reduction ratio = 9.70472847408631e-05\n",
            "pairs completeness = 0.5882352941176471\n",
            "pair quality = 0.07092198581560284\n"
          ]
        }
      ],
      "source": [
        "pair_n_in_RxR = ds1.generate_dataframe().shape[0] * ds2.generate_dataframe().shape[0]\n",
        "pair_compared = len(pairs_pred)\n",
        "true_matches_in_RxR = len(pairs_groundtrue)\n",
        "\n",
        "true_matches_compares = 0\n",
        "for pair_groundtrue in pairs_groundtrue:\n",
        "  matched = False\n",
        "  for pair_pred in pairs_pred:\n",
        "    if pair_groundtrue == pair_pred:\n",
        "      true_matches_compares += 1 # TP\n",
        "      matched = True\n",
        "      break\n",
        "\n",
        "# reduction_ratio = 1 - pair_compared/pair_n_in_RxR\n",
        "reduction_ratio = pair_compared / pair_n_in_RxR\n",
        "\n",
        "# pairs_completeness = number of true matches compares / number of true matches in RxR\n",
        "pairs_completeness = true_matches_compares / true_matches_in_RxR\n",
        "\n",
        "# pair quality = number of true matches compares / number of matches compared\n",
        "pair_quality = true_matches_compares / pair_compared\n",
        "\n",
        "print(f'reduction ratio = {reduction_ratio}\\npairs completeness = {pairs_completeness}\\npair quality = {pair_quality}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIkoz0cn7X8X",
        "outputId": "355ce372-1979-4ace-97b0-0892214477c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(141, 1452900)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pair_compared , pair_n_in_RxR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCpi3AqZJ3xR"
      },
      "source": [
        "### Task 1-3. Entity Linking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98xt8o9kJ3xS"
      },
      "source": [
        "Here are 2 example functions for field (attribute) similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ3jFoFSJ3xS",
        "outputId": "c04d3f97-765a-4edc-b3ac-a35f1ccc99a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial statistics based on Ground-Truth from development set data:\n",
            "tp: 0.750000 [12]\n",
            "fp: 0.500000 [1]\n",
            "tn: 0.500000 [1]\n",
            "fn: 0.250000 [4]\n",
            "f_measure: 0.8275862068965517\n"
          ]
        }
      ],
      "source": [
        "def jaccard_sim(a, b):\n",
        "    a, b = set(a), set(b)\n",
        "    if len(a.union(b)) == 0: return 0\n",
        "    jaccard_sim = float(len(a.intersection(b))) / len(a.union(b))\n",
        "    return jaccard_sim\n",
        "\n",
        "def singer_jaro_sim(r1, r2): \n",
        "    return rltk.jaro_winkler_similarity(r1.singer , r2.singer)\n",
        "    \n",
        "def song_jaro_sim(r1, r2):\n",
        "    return rltk.jaro_winkler_similarity(r1.song, r2.song)\n",
        "\n",
        "def last_name_jaccard_sim(r1, r2):\n",
        "    return jaccard_sim(r1.author_last_name, r2.author_last_name)\n",
        "\n",
        "def singer_tokens_levenshtein_sim(r1, r2): \n",
        "    for n1, n2 in zip(sorted(r1.singer_tokens), sorted(r2.singer_tokens)):\n",
        "        if rltk.levenshtein_distance(n1, n2) > min(len(n1), len(n2)) / 3:\n",
        "            return 0\n",
        "    return 1\n",
        "\n",
        "def singer_tokens_jaccard_sim(r1, r2):\n",
        "  return jaccard_sim(r1.singer_tokens, r2.singer_tokens)\n",
        "    \n",
        "\n",
        "# threshold value to determine if we are confident the record match\n",
        "MY_TRESH = 0.8\n",
        "# entity linkage scoring function by combining multiple sim functions into a single weightened scoring function\n",
        "def rule_based_method(r1, r2):\n",
        "    score_song_jaro = song_jaro_sim(r1, r2)\n",
        "    score_singer_jaro = singer_jaro_sim(r1, r2)\n",
        "    score_singer_tokens_levenshtein = singer_tokens_levenshtein_sim(r1, r2)\n",
        "    score_singer_tokens_jaccard = singer_tokens_jaccard_sim(r1, r2)\n",
        "\n",
        "    # total = 0.7*score_song_jaro + 0.3*score_singer_jaro\n",
        "    # total = score_singer_tokens_levenshtein # 0.7692\n",
        "    # total = score_singer_jaro # 0.769\n",
        "    total = score_song_jaro # 0.8275\n",
        "    # total = score_singer_tokens_jaccard #  0.76923\n",
        "    # total = 0.8*score_song_jaro + 0.1*score_singer_jaro + 0.1*score_singer_tokens_levenshtein # 0.8275\n",
        "    # total = 0.6*score_song_jaro + 0.2*score_singer_jaro + 0.2*score_singer_tokens_levenshtein # 0.7692\n",
        "\n",
        "    # return two values: boolean if they match or not, float to determine confidence\n",
        "    return total > MY_TRESH, total\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################\n",
        "##### Evaluation ######\n",
        "##################################\n",
        "# run some candidates using the ground-truth\n",
        "trial = rltk.Trial(gt)\n",
        "candidate_pairs = rltk.get_record_pairs(ds1, ds2, ground_truth=gt)\n",
        "for r1, r2 in candidate_pairs:\n",
        "    result, confidence = rule_based_method(r1, r2)\n",
        "    trial.add_result(r1, r2, result, confidence)\n",
        "\n",
        "# evaluation\n",
        "trial.evaluate()\n",
        "print('Trial statistics based on Ground-Truth from development set data:')\n",
        "print(f'tp: {trial.true_positives:.06f} [{len(trial.true_positives_list)}]')\n",
        "print(f'fp: {trial.false_positives:.06f} [{len(trial.false_positives_list)}]')\n",
        "print(f'tn: {trial.true_negatives:.06f} [{len(trial.true_negatives_list)}]')\n",
        "print(f'fn: {trial.false_negatives:.06f} [{len(trial.false_negatives_list)}]')\n",
        "print(f'f_measure: {trial.f_measure}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c-kCWqJ3xT"
      },
      "source": [
        "### Save Test predictions\n",
        "Test on secondhandsong and wikidata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1qkfFDrJ3xT",
        "outputId": "3f395650-bfbb-4464-d283-32bdaa079572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names are: secondhandsong_ID,  wiki_nominated_ID\n",
            "Processed 141 lines.\n"
          ]
        }
      ],
      "source": [
        "test_set_file = dir_ + 'blocked.csv'\n",
        "test = []\n",
        "with open(test_set_file, encoding='utf-8', errors=\"replace\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "        if len(row) <= 1:\n",
        "            continue\n",
        "        if line_count == 0:\n",
        "            columns = row\n",
        "            line_count += 1\n",
        "        else:\n",
        "            test.append(row)\n",
        "    print(f'Column names are: {\", \".join(columns)}')\n",
        "    print(f'Processed {len(test)} lines.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfC2TKISJ3xT",
        "outputId": "45b21704-b63e-4bcc-a8bd-c8bcba6b4ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141 10020 145\n"
          ]
        }
      ],
      "source": [
        "predictions = []\n",
        "for id1, id2 in test:\n",
        "    r1 = ds1.get_record(id1)\n",
        "    r2  = ds2.get_record(id2)\n",
        "    result, confidence = rule_based_method(r1, r2)\n",
        "    predictions.append((r1.id, r2.id, result, confidence))\n",
        "\n",
        "print(len(predictions), len(ds1.generate_dataframe()), len(ds2.generate_dataframe()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "UnxUzg4WJ3xT"
      },
      "outputs": [],
      "source": [
        "with open(dir_ + 'secondhandsong_wiki_matchings_confidence.csv', mode='w') as file:\n",
        "    file.write('secondhandsong_ID, wiki_nominated_ID, prediction, confidence\\n')\n",
        "    for row in predictions:\n",
        "      r1_id, r2_id, result, confidence = row\n",
        "      file.write(f'{r1_id}, {r2_id}, {result}, {confidence}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "TUuEg_WzncA2"
      },
      "outputs": [],
      "source": [
        "with open(dir_ + 'secondhandsong_wiki_matchings.csv', mode='w') as file:\n",
        "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    file.write('secondhandsong_ID, wiki_nominated_ID\\n')\n",
        "    for row in predictions:\n",
        "      r1_id, r2_id, result, confidence = row\n",
        "      if result:\n",
        "        file.write(f'{r1_id}, {r2_id}\\n')\n",
        "        # writer.writerow(row)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:hw1] *",
      "language": "python",
      "name": "conda-env-hw1-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}